{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 31ms/step - loss: 2.9661 - accuracy: 0.4855 - val_loss: 0.3725 - val_accuracy: 0.8492\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3901 - accuracy: 0.8655 - val_loss: 0.4494 - val_accuracy: 0.8308\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4607 - accuracy: 0.8196 - val_loss: 0.4530 - val_accuracy: 0.8262\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4067 - accuracy: 0.8412 - val_loss: 0.3431 - val_accuracy: 0.8723\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3109 - accuracy: 0.8897 - val_loss: 0.2650 - val_accuracy: 0.9062\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2673 - accuracy: 0.9130 - val_loss: 0.2540 - val_accuracy: 0.9100\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2527 - accuracy: 0.9179 - val_loss: 0.2321 - val_accuracy: 0.9192\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2209 - accuracy: 0.9253 - val_loss: 0.2108 - val_accuracy: 0.9231\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2083 - accuracy: 0.9292 - val_loss: 0.2106 - val_accuracy: 0.9262\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1997 - accuracy: 0.9338 - val_loss: 0.2024 - val_accuracy: 0.9238\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1933 - accuracy: 0.9351 - val_loss: 0.1986 - val_accuracy: 0.9269\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1893 - accuracy: 0.9364 - val_loss: 0.1962 - val_accuracy: 0.9292\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1863 - accuracy: 0.9371 - val_loss: 0.1939 - val_accuracy: 0.9300\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1837 - accuracy: 0.9382 - val_loss: 0.1925 - val_accuracy: 0.9292\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1818 - accuracy: 0.9371 - val_loss: 0.1900 - val_accuracy: 0.9315\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1805 - accuracy: 0.9374 - val_loss: 0.1898 - val_accuracy: 0.9300\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1792 - accuracy: 0.9387 - val_loss: 0.1880 - val_accuracy: 0.9323\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1779 - accuracy: 0.9382 - val_loss: 0.1854 - val_accuracy: 0.9346\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1769 - accuracy: 0.9379 - val_loss: 0.1857 - val_accuracy: 0.9331\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1758 - accuracy: 0.9397 - val_loss: 0.1852 - val_accuracy: 0.9315\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1745 - accuracy: 0.9400 - val_loss: 0.1817 - val_accuracy: 0.9362\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1738 - accuracy: 0.9382 - val_loss: 0.1807 - val_accuracy: 0.9362\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1725 - accuracy: 0.9402 - val_loss: 0.1795 - val_accuracy: 0.9369\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1717 - accuracy: 0.9394 - val_loss: 0.1772 - val_accuracy: 0.9377\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1707 - accuracy: 0.9392 - val_loss: 0.1756 - val_accuracy: 0.9369\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1696 - accuracy: 0.9397 - val_loss: 0.1734 - val_accuracy: 0.9369\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1690 - accuracy: 0.9402 - val_loss: 0.1724 - val_accuracy: 0.9369\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1686 - accuracy: 0.9397 - val_loss: 0.1741 - val_accuracy: 0.9346\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1676 - accuracy: 0.9394 - val_loss: 0.1715 - val_accuracy: 0.9369\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1673 - accuracy: 0.9397 - val_loss: 0.1727 - val_accuracy: 0.9369\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1662 - accuracy: 0.9405 - val_loss: 0.1693 - val_accuracy: 0.9377\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1658 - accuracy: 0.9402 - val_loss: 0.1717 - val_accuracy: 0.9354\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1653 - accuracy: 0.9410 - val_loss: 0.1691 - val_accuracy: 0.9377\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1642 - accuracy: 0.9415 - val_loss: 0.1695 - val_accuracy: 0.9377\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1640 - accuracy: 0.9412 - val_loss: 0.1676 - val_accuracy: 0.9377\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1635 - accuracy: 0.9415 - val_loss: 0.1667 - val_accuracy: 0.9377\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1627 - accuracy: 0.9418 - val_loss: 0.1676 - val_accuracy: 0.9385\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1624 - accuracy: 0.9420 - val_loss: 0.1652 - val_accuracy: 0.9385\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1613 - accuracy: 0.9430 - val_loss: 0.1655 - val_accuracy: 0.9392\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1610 - accuracy: 0.9418 - val_loss: 0.1653 - val_accuracy: 0.9392\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1603 - accuracy: 0.9420 - val_loss: 0.1646 - val_accuracy: 0.9400\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1596 - accuracy: 0.9420 - val_loss: 0.1624 - val_accuracy: 0.9392\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1591 - accuracy: 0.9435 - val_loss: 0.1631 - val_accuracy: 0.9392\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1585 - accuracy: 0.9428 - val_loss: 0.1636 - val_accuracy: 0.9385\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1577 - accuracy: 0.9435 - val_loss: 0.1613 - val_accuracy: 0.9408\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1572 - accuracy: 0.9433 - val_loss: 0.1613 - val_accuracy: 0.9392\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1564 - accuracy: 0.9441 - val_loss: 0.1599 - val_accuracy: 0.9400\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1556 - accuracy: 0.9435 - val_loss: 0.1585 - val_accuracy: 0.9400\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1550 - accuracy: 0.9446 - val_loss: 0.1584 - val_accuracy: 0.9392\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1550 - accuracy: 0.9443 - val_loss: 0.1586 - val_accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 4ms/step - loss: 0.1686 - accuracy: 0.9385\n",
      "Test accuracy: 0.9384615421295166\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model/all\\01-0.2377.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model/all\\02-0.8308.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model/all\\03-0.8292.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model/all\\04-0.8477.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model/all\\05-0.8823.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model/all\\06-0.9031.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model/all\\07-0.9038.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model/all\\08-0.9108.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model/all\\09-0.9200.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model/all\\10-0.9192.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model/all\\11-0.9200.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model/all\\12-0.9192.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model/all\\13-0.9208.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model/all\\14-0.9208.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model/all\\15-0.9215.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model/all\\16-0.9200.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model/all\\17-0.9215.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model/all\\18-0.9215.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model/all\\19-0.9215.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model/all\\20-0.9215.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model/all\\21-0.9223.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model/all\\22-0.9215.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model/all\\23-0.9215.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model/all\\24-0.9223.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model/all\\25-0.9223.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model/all\\26-0.9223.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model/all\\27-0.9231.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model/all\\28-0.9246.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model/all\\29-0.9223.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model/all\\30-0.9231.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model/all\\31-0.9231.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model/all\\32-0.9223.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model/all\\33-0.9262.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model/all\\34-0.9262.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model/all\\35-0.9262.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model/all\\36-0.9262.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model/all\\37-0.9262.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model/all\\38-0.9262.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model/all\\39-0.9262.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model/all\\40-0.9269.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model/all\\41-0.9262.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model/all\\42-0.9285.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model/all\\43-0.9269.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model/all\\44-0.9269.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model/all\\45-0.9292.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model/all\\46-0.9269.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model/all\\47-0.9277.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model/all\\48-0.9277.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model/all\\49-0.9277.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model/all\\50-0.9277.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 5ms/step - loss: 0.1925 - accuracy: 0.9362\n",
      "Test accuracy: 0.9361538290977478\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.189184</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>0.208974</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.187946</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>0.209243</td>\n",
       "      <td>0.926923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.187518</td>\n",
       "      <td>0.937388</td>\n",
       "      <td>0.208360</td>\n",
       "      <td>0.928462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.186824</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>0.207440</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.186251</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>0.207259</td>\n",
       "      <td>0.928462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.019946</td>\n",
       "      <td>0.994098</td>\n",
       "      <td>0.144977</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.020857</td>\n",
       "      <td>0.993841</td>\n",
       "      <td>0.143104</td>\n",
       "      <td>0.983077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.020986</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.144777</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.021375</td>\n",
       "      <td>0.993585</td>\n",
       "      <td>0.146978</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.020309</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.144927</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.189184  0.937644  0.208974      0.930000\n",
       "1     0.187946  0.937644  0.209243      0.926923\n",
       "2     0.187518  0.937388  0.208360      0.928462\n",
       "3     0.186824  0.937644  0.207440      0.930000\n",
       "4     0.186251  0.937644  0.207259      0.928462\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.019946  0.994098  0.144977      0.980000\n",
       "1996  0.020857  0.993841  0.143104      0.983077\n",
       "1997  0.020986  0.994355  0.144777      0.980769\n",
       "1998  0.021375  0.993585  0.146978      0.980000\n",
       "1999  0.020309  0.994355  0.144927      0.980769\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 30)                390       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 12)                372       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 37ms/step - loss: 1.0284 - accuracy: 0.7496 - val_loss: 0.6208 - val_accuracy: 0.7746\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.5379 - accuracy: 0.7501 - val_loss: 0.4496 - val_accuracy: 0.7800\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.4037 - accuracy: 0.7729 - val_loss: 0.3385 - val_accuracy: 0.8023\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.3499 - accuracy: 0.8006 - val_loss: 0.3074 - val_accuracy: 0.8500\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.3168 - accuracy: 0.8791 - val_loss: 0.2954 - val_accuracy: 0.8992\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2962 - accuracy: 0.8945 - val_loss: 0.2768 - val_accuracy: 0.9038\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.2786 - accuracy: 0.9063 - val_loss: 0.2672 - val_accuracy: 0.9192\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2649 - accuracy: 0.9156 - val_loss: 0.2549 - val_accuracy: 0.9215\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2530 - accuracy: 0.9225 - val_loss: 0.2483 - val_accuracy: 0.9215\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2445 - accuracy: 0.9253 - val_loss: 0.2408 - val_accuracy: 0.9223\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2374 - accuracy: 0.9266 - val_loss: 0.2369 - val_accuracy: 0.9238\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2324 - accuracy: 0.9317 - val_loss: 0.2319 - val_accuracy: 0.9246\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2269 - accuracy: 0.9287 - val_loss: 0.2272 - val_accuracy: 0.9246\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2201 - accuracy: 0.9328 - val_loss: 0.2236 - val_accuracy: 0.9269\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.2156 - accuracy: 0.9341 - val_loss: 0.2202 - val_accuracy: 0.9292\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2085 - accuracy: 0.9343 - val_loss: 0.2117 - val_accuracy: 0.9277\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1988 - accuracy: 0.9343 - val_loss: 0.2164 - val_accuracy: 0.9254\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1957 - accuracy: 0.9361 - val_loss: 0.2065 - val_accuracy: 0.9292\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1925 - accuracy: 0.9356 - val_loss: 0.2066 - val_accuracy: 0.9277\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.1893 - accuracy: 0.9348 - val_loss: 0.2058 - val_accuracy: 0.9269\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1872 - accuracy: 0.9351 - val_loss: 0.2016 - val_accuracy: 0.9292\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1856 - accuracy: 0.9366 - val_loss: 0.1991 - val_accuracy: 0.9292\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1835 - accuracy: 0.9382 - val_loss: 0.1990 - val_accuracy: 0.9277\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1810 - accuracy: 0.9402 - val_loss: 0.1968 - val_accuracy: 0.9285\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1792 - accuracy: 0.9410 - val_loss: 0.1952 - val_accuracy: 0.9300\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1775 - accuracy: 0.9423 - val_loss: 0.1930 - val_accuracy: 0.9300\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1761 - accuracy: 0.9423 - val_loss: 0.1925 - val_accuracy: 0.9331\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1749 - accuracy: 0.9425 - val_loss: 0.1899 - val_accuracy: 0.9331\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1740 - accuracy: 0.9415 - val_loss: 0.1879 - val_accuracy: 0.9323\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1731 - accuracy: 0.9433 - val_loss: 0.1871 - val_accuracy: 0.9338\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1700 - accuracy: 0.9446 - val_loss: 0.1869 - val_accuracy: 0.9338\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1690 - accuracy: 0.9448 - val_loss: 0.1866 - val_accuracy: 0.9338\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1678 - accuracy: 0.9453 - val_loss: 0.1832 - val_accuracy: 0.9362\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1661 - accuracy: 0.9451 - val_loss: 0.1813 - val_accuracy: 0.9362\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1646 - accuracy: 0.9448 - val_loss: 0.1817 - val_accuracy: 0.9354\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1632 - accuracy: 0.9459 - val_loss: 0.1813 - val_accuracy: 0.9346\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1624 - accuracy: 0.9451 - val_loss: 0.1826 - val_accuracy: 0.9338\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1616 - accuracy: 0.9451 - val_loss: 0.1756 - val_accuracy: 0.9400\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1589 - accuracy: 0.9484 - val_loss: 0.1751 - val_accuracy: 0.9392\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1569 - accuracy: 0.9466 - val_loss: 0.1737 - val_accuracy: 0.9385\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1556 - accuracy: 0.9471 - val_loss: 0.1728 - val_accuracy: 0.9400\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1535 - accuracy: 0.9464 - val_loss: 0.1714 - val_accuracy: 0.9400\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1509 - accuracy: 0.9482 - val_loss: 0.1714 - val_accuracy: 0.9362\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1490 - accuracy: 0.9482 - val_loss: 0.1662 - val_accuracy: 0.9385\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1457 - accuracy: 0.9489 - val_loss: 0.1642 - val_accuracy: 0.9408\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1444 - accuracy: 0.9500 - val_loss: 0.1622 - val_accuracy: 0.9408\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1409 - accuracy: 0.9502 - val_loss: 0.1607 - val_accuracy: 0.9377\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1396 - accuracy: 0.9492 - val_loss: 0.1579 - val_accuracy: 0.9385\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1367 - accuracy: 0.9515 - val_loss: 0.1565 - val_accuracy: 0.9415\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1365 - accuracy: 0.9518 - val_loss: 0.1585 - val_accuracy: 0.9446\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1364 - accuracy: 0.9487 - val_loss: 0.1535 - val_accuracy: 0.9392\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1308 - accuracy: 0.9528 - val_loss: 0.1591 - val_accuracy: 0.9369\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1299 - accuracy: 0.9533 - val_loss: 0.1516 - val_accuracy: 0.9377\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1275 - accuracy: 0.9520 - val_loss: 0.1489 - val_accuracy: 0.9400\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1256 - accuracy: 0.9546 - val_loss: 0.1467 - val_accuracy: 0.9438\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1240 - accuracy: 0.9533 - val_loss: 0.1481 - val_accuracy: 0.9385\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1239 - accuracy: 0.9541 - val_loss: 0.1478 - val_accuracy: 0.9408\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1225 - accuracy: 0.9541 - val_loss: 0.1479 - val_accuracy: 0.9415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1201 - accuracy: 0.9574 - val_loss: 0.1424 - val_accuracy: 0.9431\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1192 - accuracy: 0.9566 - val_loss: 0.1420 - val_accuracy: 0.9423\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1208 - accuracy: 0.9530 - val_loss: 0.1422 - val_accuracy: 0.9423\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1190 - accuracy: 0.9566 - val_loss: 0.1520 - val_accuracy: 0.9423\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1175 - accuracy: 0.9582 - val_loss: 0.1390 - val_accuracy: 0.9454\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1141 - accuracy: 0.9597 - val_loss: 0.1364 - val_accuracy: 0.9469\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1122 - accuracy: 0.9618 - val_loss: 0.1359 - val_accuracy: 0.9462\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1112 - accuracy: 0.9620 - val_loss: 0.1355 - val_accuracy: 0.9462\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1108 - accuracy: 0.9610 - val_loss: 0.1346 - val_accuracy: 0.9485\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1097 - accuracy: 0.9636 - val_loss: 0.1333 - val_accuracy: 0.9469\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1084 - accuracy: 0.9600 - val_loss: 0.1360 - val_accuracy: 0.9492\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1094 - accuracy: 0.9620 - val_loss: 0.1443 - val_accuracy: 0.9469\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1093 - accuracy: 0.9630 - val_loss: 0.1329 - val_accuracy: 0.9508\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1066 - accuracy: 0.9636 - val_loss: 0.1305 - val_accuracy: 0.9485\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1047 - accuracy: 0.9651 - val_loss: 0.1331 - val_accuracy: 0.9500\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1051 - accuracy: 0.9636 - val_loss: 0.1279 - val_accuracy: 0.9508\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1081 - accuracy: 0.9630 - val_loss: 0.1298 - val_accuracy: 0.9515\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1033 - accuracy: 0.9659 - val_loss: 0.1280 - val_accuracy: 0.9508\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1012 - accuracy: 0.9651 - val_loss: 0.1271 - val_accuracy: 0.9531\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1000 - accuracy: 0.9654 - val_loss: 0.1270 - val_accuracy: 0.9538\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0992 - accuracy: 0.9664 - val_loss: 0.1254 - val_accuracy: 0.9523\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1011 - accuracy: 0.9638 - val_loss: 0.1258 - val_accuracy: 0.9531\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0979 - accuracy: 0.9687 - val_loss: 0.1238 - val_accuracy: 0.9531\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0986 - accuracy: 0.9674 - val_loss: 0.1285 - val_accuracy: 0.9531\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0958 - accuracy: 0.9674 - val_loss: 0.1221 - val_accuracy: 0.9546\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0978 - accuracy: 0.9666 - val_loss: 0.1243 - val_accuracy: 0.9523\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0953 - accuracy: 0.9659 - val_loss: 0.1214 - val_accuracy: 0.9562\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0955 - accuracy: 0.9669 - val_loss: 0.1206 - val_accuracy: 0.9569\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0947 - accuracy: 0.9666 - val_loss: 0.1195 - val_accuracy: 0.9577\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0938 - accuracy: 0.9674 - val_loss: 0.1190 - val_accuracy: 0.9562\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0920 - accuracy: 0.9679 - val_loss: 0.1213 - val_accuracy: 0.9600\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0918 - accuracy: 0.9669 - val_loss: 0.1191 - val_accuracy: 0.9585\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0903 - accuracy: 0.9700 - val_loss: 0.1172 - val_accuracy: 0.9562\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0907 - accuracy: 0.9690 - val_loss: 0.1164 - val_accuracy: 0.9592\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0892 - accuracy: 0.9687 - val_loss: 0.1166 - val_accuracy: 0.9577\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0886 - accuracy: 0.9692 - val_loss: 0.1148 - val_accuracy: 0.9577\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0883 - accuracy: 0.9713 - val_loss: 0.1161 - val_accuracy: 0.9562\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0888 - accuracy: 0.9713 - val_loss: 0.1136 - val_accuracy: 0.9600\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0876 - accuracy: 0.9702 - val_loss: 0.1138 - val_accuracy: 0.9600\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0860 - accuracy: 0.9705 - val_loss: 0.1128 - val_accuracy: 0.9615\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0869 - accuracy: 0.9700 - val_loss: 0.1120 - val_accuracy: 0.9600\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0863 - accuracy: 0.9715 - val_loss: 0.1124 - val_accuracy: 0.9646\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0845 - accuracy: 0.9723 - val_loss: 0.1228 - val_accuracy: 0.9615\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0903 - accuracy: 0.9684 - val_loss: 0.1163 - val_accuracy: 0.9638\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0900 - accuracy: 0.9713 - val_loss: 0.1098 - val_accuracy: 0.9623\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0854 - accuracy: 0.9723 - val_loss: 0.1133 - val_accuracy: 0.9585\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0825 - accuracy: 0.9733 - val_loss: 0.1098 - val_accuracy: 0.9669\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0819 - accuracy: 0.9733 - val_loss: 0.1090 - val_accuracy: 0.9669\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0809 - accuracy: 0.9731 - val_loss: 0.1064 - val_accuracy: 0.9662\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0810 - accuracy: 0.9738 - val_loss: 0.1122 - val_accuracy: 0.9638\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0826 - accuracy: 0.9733 - val_loss: 0.1057 - val_accuracy: 0.9677\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0789 - accuracy: 0.9761 - val_loss: 0.1050 - val_accuracy: 0.9623\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0793 - accuracy: 0.9743 - val_loss: 0.1053 - val_accuracy: 0.9623\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0795 - accuracy: 0.9746 - val_loss: 0.1029 - val_accuracy: 0.9654\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0782 - accuracy: 0.9759 - val_loss: 0.1057 - val_accuracy: 0.9646\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0785 - accuracy: 0.9738 - val_loss: 0.1034 - val_accuracy: 0.9692\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0782 - accuracy: 0.9743 - val_loss: 0.1018 - val_accuracy: 0.9685\n",
      "Epoch 116/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0762 - accuracy: 0.9774 - val_loss: 0.1017 - val_accuracy: 0.9662\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0754 - accuracy: 0.9769 - val_loss: 0.1007 - val_accuracy: 0.9692\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0748 - accuracy: 0.9764 - val_loss: 0.1000 - val_accuracy: 0.9677\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0750 - accuracy: 0.9769 - val_loss: 0.1008 - val_accuracy: 0.9654\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0758 - accuracy: 0.9754 - val_loss: 0.1001 - val_accuracy: 0.9654\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0740 - accuracy: 0.9764 - val_loss: 0.0981 - val_accuracy: 0.9708\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0732 - accuracy: 0.9784 - val_loss: 0.0982 - val_accuracy: 0.9700\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0748 - accuracy: 0.9756 - val_loss: 0.0976 - val_accuracy: 0.9692\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0765 - accuracy: 0.9749 - val_loss: 0.0969 - val_accuracy: 0.9715\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0771 - accuracy: 0.9736 - val_loss: 0.1025 - val_accuracy: 0.9669\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0717 - accuracy: 0.9779 - val_loss: 0.0952 - val_accuracy: 0.9723\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0709 - accuracy: 0.9779 - val_loss: 0.0952 - val_accuracy: 0.9715\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0697 - accuracy: 0.9792 - val_loss: 0.0956 - val_accuracy: 0.9715\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0699 - accuracy: 0.9797 - val_loss: 0.0964 - val_accuracy: 0.9708\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0690 - accuracy: 0.9797 - val_loss: 0.0949 - val_accuracy: 0.9723\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0684 - accuracy: 0.9810 - val_loss: 0.0935 - val_accuracy: 0.9715\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0676 - accuracy: 0.9782 - val_loss: 0.0927 - val_accuracy: 0.9738\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0678 - accuracy: 0.9795 - val_loss: 0.0967 - val_accuracy: 0.9708\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0658 - accuracy: 0.9808 - val_loss: 0.0922 - val_accuracy: 0.9715\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0664 - accuracy: 0.9810 - val_loss: 0.0894 - val_accuracy: 0.9738\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0646 - accuracy: 0.9813 - val_loss: 0.0910 - val_accuracy: 0.9715\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0644 - accuracy: 0.9820 - val_loss: 0.0930 - val_accuracy: 0.9723\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0676 - accuracy: 0.9792 - val_loss: 0.0884 - val_accuracy: 0.9723\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0636 - accuracy: 0.9815 - val_loss: 0.0899 - val_accuracy: 0.9731\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0630 - accuracy: 0.9815 - val_loss: 0.0892 - val_accuracy: 0.9715\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0665 - accuracy: 0.9800 - val_loss: 0.0915 - val_accuracy: 0.9738\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0650 - accuracy: 0.9795 - val_loss: 0.0882 - val_accuracy: 0.9723\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0643 - accuracy: 0.9805 - val_loss: 0.0870 - val_accuracy: 0.9746\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0616 - accuracy: 0.9813 - val_loss: 0.0861 - val_accuracy: 0.9738\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0613 - accuracy: 0.9805 - val_loss: 0.0855 - val_accuracy: 0.9746\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0609 - accuracy: 0.9818 - val_loss: 0.0847 - val_accuracy: 0.9723\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0606 - accuracy: 0.9818 - val_loss: 0.0850 - val_accuracy: 0.9723\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0609 - accuracy: 0.9802 - val_loss: 0.0854 - val_accuracy: 0.9754\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0593 - accuracy: 0.9828 - val_loss: 0.0845 - val_accuracy: 0.9746\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0592 - accuracy: 0.9828 - val_loss: 0.0831 - val_accuracy: 0.9746\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0585 - accuracy: 0.9833 - val_loss: 0.0893 - val_accuracy: 0.9746\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0617 - accuracy: 0.9823 - val_loss: 0.0841 - val_accuracy: 0.9754\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0581 - accuracy: 0.9841 - val_loss: 0.0818 - val_accuracy: 0.9762\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0610 - accuracy: 0.9813 - val_loss: 0.0855 - val_accuracy: 0.9738\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0570 - accuracy: 0.9849 - val_loss: 0.0825 - val_accuracy: 0.9754\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0581 - accuracy: 0.9820 - val_loss: 0.0858 - val_accuracy: 0.9738\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0583 - accuracy: 0.9831 - val_loss: 0.0866 - val_accuracy: 0.9738\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0574 - accuracy: 0.9831 - val_loss: 0.0830 - val_accuracy: 0.9754\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0569 - accuracy: 0.9838 - val_loss: 0.0810 - val_accuracy: 0.9769\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0571 - accuracy: 0.9841 - val_loss: 0.0792 - val_accuracy: 0.9785\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0565 - accuracy: 0.9838 - val_loss: 0.0787 - val_accuracy: 0.9792\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0556 - accuracy: 0.9838 - val_loss: 0.0809 - val_accuracy: 0.9762\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0787 - val_accuracy: 0.9777\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0554 - accuracy: 0.9841 - val_loss: 0.0799 - val_accuracy: 0.9769\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0545 - accuracy: 0.9849 - val_loss: 0.0773 - val_accuracy: 0.9815\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0537 - accuracy: 0.9854 - val_loss: 0.0785 - val_accuracy: 0.9792\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0529 - accuracy: 0.9856 - val_loss: 0.0763 - val_accuracy: 0.9800\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9849 - val_loss: 0.0782 - val_accuracy: 0.9762\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0535 - accuracy: 0.9838 - val_loss: 0.0801 - val_accuracy: 0.9769\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0539 - accuracy: 0.9838 - val_loss: 0.0767 - val_accuracy: 0.9785\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0526 - accuracy: 0.9849 - val_loss: 0.0767 - val_accuracy: 0.9792\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0516 - accuracy: 0.9849 - val_loss: 0.0772 - val_accuracy: 0.9777\n",
      "Epoch 173/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0517 - accuracy: 0.9854 - val_loss: 0.0748 - val_accuracy: 0.9823\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0521 - accuracy: 0.9851 - val_loss: 0.0746 - val_accuracy: 0.9792\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0541 - accuracy: 0.9841 - val_loss: 0.0761 - val_accuracy: 0.9800\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0517 - accuracy: 0.9859 - val_loss: 0.0773 - val_accuracy: 0.9777\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0529 - accuracy: 0.9843 - val_loss: 0.0877 - val_accuracy: 0.9754\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0562 - accuracy: 0.9831 - val_loss: 0.0751 - val_accuracy: 0.9800\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0544 - accuracy: 0.9846 - val_loss: 0.0744 - val_accuracy: 0.9823\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0739 - val_accuracy: 0.9800\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0510 - accuracy: 0.9859 - val_loss: 0.0745 - val_accuracy: 0.9792\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9867 - val_loss: 0.0743 - val_accuracy: 0.9777\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0494 - accuracy: 0.9859 - val_loss: 0.0737 - val_accuracy: 0.9823\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0513 - accuracy: 0.9843 - val_loss: 0.0813 - val_accuracy: 0.9754\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0511 - accuracy: 0.9856 - val_loss: 0.0729 - val_accuracy: 0.9815\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0489 - accuracy: 0.9856 - val_loss: 0.0737 - val_accuracy: 0.9792\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0497 - accuracy: 0.9867 - val_loss: 0.0817 - val_accuracy: 0.9769\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0544 - accuracy: 0.9854 - val_loss: 0.0731 - val_accuracy: 0.9785\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0498 - accuracy: 0.9867 - val_loss: 0.0732 - val_accuracy: 0.9800\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0489 - accuracy: 0.9864 - val_loss: 0.0726 - val_accuracy: 0.9831\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.9877 - val_loss: 0.0731 - val_accuracy: 0.9831\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0507 - accuracy: 0.9846 - val_loss: 0.0751 - val_accuracy: 0.9769\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0492 - accuracy: 0.9869 - val_loss: 0.0748 - val_accuracy: 0.9785\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0488 - accuracy: 0.9872 - val_loss: 0.0725 - val_accuracy: 0.9808\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 0.9864 - val_loss: 0.0737 - val_accuracy: 0.9800\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0507 - accuracy: 0.9849 - val_loss: 0.0716 - val_accuracy: 0.9800\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0495 - accuracy: 0.9874 - val_loss: 0.0737 - val_accuracy: 0.9792\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0476 - accuracy: 0.9882 - val_loss: 0.0731 - val_accuracy: 0.9808\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0480 - accuracy: 0.9877 - val_loss: 0.0708 - val_accuracy: 0.9823\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0462 - accuracy: 0.9877 - val_loss: 0.0712 - val_accuracy: 0.9823\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0465 - accuracy: 0.9892 - val_loss: 0.0711 - val_accuracy: 0.9823\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0465 - accuracy: 0.9879 - val_loss: 0.0714 - val_accuracy: 0.9831\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0460 - accuracy: 0.9879 - val_loss: 0.0711 - val_accuracy: 0.9831\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0464 - accuracy: 0.9882 - val_loss: 0.0715 - val_accuracy: 0.9808\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0465 - accuracy: 0.9892 - val_loss: 0.0764 - val_accuracy: 0.9777\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0471 - accuracy: 0.9867 - val_loss: 0.0709 - val_accuracy: 0.9831\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0456 - accuracy: 0.9877 - val_loss: 0.0696 - val_accuracy: 0.9823\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0461 - accuracy: 0.9885 - val_loss: 0.0704 - val_accuracy: 0.9831\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0459 - accuracy: 0.9885 - val_loss: 0.0723 - val_accuracy: 0.9808\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0464 - accuracy: 0.9874 - val_loss: 0.0709 - val_accuracy: 0.9823\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0552 - accuracy: 0.9851 - val_loss: 0.0800 - val_accuracy: 0.9769\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0490 - accuracy: 0.9864 - val_loss: 0.0714 - val_accuracy: 0.9823\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0460 - accuracy: 0.9877 - val_loss: 0.0700 - val_accuracy: 0.9815\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0453 - accuracy: 0.9890 - val_loss: 0.0701 - val_accuracy: 0.9808\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0441 - accuracy: 0.9895 - val_loss: 0.0729 - val_accuracy: 0.9792\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0495 - accuracy: 0.9877 - val_loss: 0.0795 - val_accuracy: 0.9762\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0546 - accuracy: 0.9831 - val_loss: 0.0704 - val_accuracy: 0.9831\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0497 - accuracy: 0.9856 - val_loss: 0.0709 - val_accuracy: 0.9815\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0464 - accuracy: 0.9877 - val_loss: 0.0711 - val_accuracy: 0.9823\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0449 - accuracy: 0.9887 - val_loss: 0.0704 - val_accuracy: 0.9815\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0439 - accuracy: 0.9897 - val_loss: 0.0693 - val_accuracy: 0.9823\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0440 - accuracy: 0.9895 - val_loss: 0.0702 - val_accuracy: 0.9823\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0448 - accuracy: 0.9890 - val_loss: 0.0704 - val_accuracy: 0.9808\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0440 - accuracy: 0.9892 - val_loss: 0.0681 - val_accuracy: 0.9823\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9879 - val_loss: 0.0687 - val_accuracy: 0.9823\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0437 - accuracy: 0.9892 - val_loss: 0.0727 - val_accuracy: 0.9792\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0450 - accuracy: 0.9879 - val_loss: 0.0691 - val_accuracy: 0.9823\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0429 - accuracy: 0.9895 - val_loss: 0.0682 - val_accuracy: 0.9838\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0428 - accuracy: 0.9892 - val_loss: 0.0683 - val_accuracy: 0.9831\n",
      "Epoch 230/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9900 - val_loss: 0.0698 - val_accuracy: 0.9808\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0433 - accuracy: 0.9887 - val_loss: 0.0691 - val_accuracy: 0.9831\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0427 - accuracy: 0.9900 - val_loss: 0.0679 - val_accuracy: 0.9815\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0436 - accuracy: 0.9882 - val_loss: 0.0749 - val_accuracy: 0.9777\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0471 - accuracy: 0.9869 - val_loss: 0.0687 - val_accuracy: 0.9815\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0445 - accuracy: 0.9885 - val_loss: 0.0729 - val_accuracy: 0.9800\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0450 - accuracy: 0.9887 - val_loss: 0.0712 - val_accuracy: 0.9792\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9905 - val_loss: 0.0698 - val_accuracy: 0.9792\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0435 - accuracy: 0.9897 - val_loss: 0.0679 - val_accuracy: 0.9831\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0417 - accuracy: 0.9900 - val_loss: 0.0678 - val_accuracy: 0.9831\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0418 - accuracy: 0.9897 - val_loss: 0.0682 - val_accuracy: 0.9823\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0413 - accuracy: 0.9892 - val_loss: 0.0679 - val_accuracy: 0.9823\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0421 - accuracy: 0.9908 - val_loss: 0.0690 - val_accuracy: 0.9815\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0426 - accuracy: 0.9897 - val_loss: 0.0696 - val_accuracy: 0.9808\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0432 - accuracy: 0.9882 - val_loss: 0.0693 - val_accuracy: 0.9823\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0412 - accuracy: 0.9897 - val_loss: 0.0673 - val_accuracy: 0.9823\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0412 - accuracy: 0.9905 - val_loss: 0.0676 - val_accuracy: 0.9823\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0417 - accuracy: 0.9895 - val_loss: 0.0679 - val_accuracy: 0.9815\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0410 - accuracy: 0.9890 - val_loss: 0.0686 - val_accuracy: 0.9831\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0411 - accuracy: 0.9887 - val_loss: 0.0673 - val_accuracy: 0.9823\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0410 - accuracy: 0.9902 - val_loss: 0.0721 - val_accuracy: 0.9792\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0415 - accuracy: 0.9897 - val_loss: 0.0673 - val_accuracy: 0.9815\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9902 - val_loss: 0.0674 - val_accuracy: 0.9823\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9902 - val_loss: 0.0674 - val_accuracy: 0.9815\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0412 - accuracy: 0.9900 - val_loss: 0.0685 - val_accuracy: 0.9815\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 0.9902 - val_loss: 0.0679 - val_accuracy: 0.9815\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0416 - accuracy: 0.9895 - val_loss: 0.0695 - val_accuracy: 0.9808\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9892 - val_loss: 0.0673 - val_accuracy: 0.9823\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9908 - val_loss: 0.0687 - val_accuracy: 0.9815\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9910 - val_loss: 0.0687 - val_accuracy: 0.9823\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0431 - accuracy: 0.9902 - val_loss: 0.0681 - val_accuracy: 0.9823\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0429 - accuracy: 0.9905 - val_loss: 0.0702 - val_accuracy: 0.9808\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0409 - accuracy: 0.9890 - val_loss: 0.0664 - val_accuracy: 0.9823\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0418 - accuracy: 0.9892 - val_loss: 0.0673 - val_accuracy: 0.9815\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0400 - accuracy: 0.9908 - val_loss: 0.0672 - val_accuracy: 0.9815\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0393 - accuracy: 0.9910 - val_loss: 0.0661 - val_accuracy: 0.9815\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0390 - accuracy: 0.9905 - val_loss: 0.0669 - val_accuracy: 0.9815\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9902 - val_loss: 0.0664 - val_accuracy: 0.9831\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0405 - accuracy: 0.9900 - val_loss: 0.0673 - val_accuracy: 0.9815\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0390 - accuracy: 0.9905 - val_loss: 0.0664 - val_accuracy: 0.9823\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0405 - accuracy: 0.9897 - val_loss: 0.0672 - val_accuracy: 0.9815\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0388 - accuracy: 0.9908 - val_loss: 0.0653 - val_accuracy: 0.9815\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 0.0658 - val_accuracy: 0.9815\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0388 - accuracy: 0.9910 - val_loss: 0.0717 - val_accuracy: 0.9815\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0420 - accuracy: 0.9897 - val_loss: 0.0696 - val_accuracy: 0.9815\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0400 - accuracy: 0.9910 - val_loss: 0.0657 - val_accuracy: 0.9815\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0385 - accuracy: 0.9905 - val_loss: 0.0656 - val_accuracy: 0.9808\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0388 - accuracy: 0.9905 - val_loss: 0.0661 - val_accuracy: 0.9815\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0383 - accuracy: 0.9915 - val_loss: 0.0652 - val_accuracy: 0.9808\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0383 - accuracy: 0.9910 - val_loss: 0.0654 - val_accuracy: 0.9823\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 0.0650 - val_accuracy: 0.9815\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0383 - accuracy: 0.9910 - val_loss: 0.0650 - val_accuracy: 0.9815\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0381 - accuracy: 0.9908 - val_loss: 0.0657 - val_accuracy: 0.9815\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0378 - accuracy: 0.9918 - val_loss: 0.0646 - val_accuracy: 0.9815\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 0.9910 - val_loss: 0.0647 - val_accuracy: 0.9823\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0381 - accuracy: 0.9908 - val_loss: 0.0663 - val_accuracy: 0.9808\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0377 - accuracy: 0.9913 - val_loss: 0.0647 - val_accuracy: 0.9823\n",
      "Epoch 287/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0376 - accuracy: 0.9910 - val_loss: 0.0669 - val_accuracy: 0.9823\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0384 - accuracy: 0.9908 - val_loss: 0.0655 - val_accuracy: 0.9815\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0375 - accuracy: 0.9910 - val_loss: 0.0653 - val_accuracy: 0.9815\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0399 - accuracy: 0.9905 - val_loss: 0.0659 - val_accuracy: 0.9815\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0406 - accuracy: 0.9910 - val_loss: 0.0673 - val_accuracy: 0.9831\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0367 - accuracy: 0.9920 - val_loss: 0.0647 - val_accuracy: 0.9815\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0380 - accuracy: 0.9905 - val_loss: 0.0651 - val_accuracy: 0.9808\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0377 - accuracy: 0.9908 - val_loss: 0.0650 - val_accuracy: 0.9831\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 0.0658 - val_accuracy: 0.9823\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0406 - accuracy: 0.9902 - val_loss: 0.0731 - val_accuracy: 0.9815\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0374 - accuracy: 0.9918 - val_loss: 0.0641 - val_accuracy: 0.9815\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0387 - accuracy: 0.9900 - val_loss: 0.0638 - val_accuracy: 0.9823\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0366 - accuracy: 0.9920 - val_loss: 0.0663 - val_accuracy: 0.9831\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0367 - accuracy: 0.9908 - val_loss: 0.0652 - val_accuracy: 0.9823\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0383 - accuracy: 0.9908 - val_loss: 0.0728 - val_accuracy: 0.9815\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0407 - accuracy: 0.9905 - val_loss: 0.0646 - val_accuracy: 0.9808\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0406 - accuracy: 0.9905 - val_loss: 0.0631 - val_accuracy: 0.9823\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0385 - accuracy: 0.9902 - val_loss: 0.0784 - val_accuracy: 0.9792\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0404 - accuracy: 0.9908 - val_loss: 0.0656 - val_accuracy: 0.9838\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0389 - accuracy: 0.9902 - val_loss: 0.0638 - val_accuracy: 0.9831\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9900 - val_loss: 0.0660 - val_accuracy: 0.9831\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0407 - accuracy: 0.9890 - val_loss: 0.0666 - val_accuracy: 0.9831\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0393 - accuracy: 0.9897 - val_loss: 0.0641 - val_accuracy: 0.9831\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0367 - accuracy: 0.9910 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0366 - accuracy: 0.9915 - val_loss: 0.0647 - val_accuracy: 0.9831\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0366 - accuracy: 0.9913 - val_loss: 0.0630 - val_accuracy: 0.9831\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0356 - accuracy: 0.9923 - val_loss: 0.0627 - val_accuracy: 0.9815\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0367 - accuracy: 0.9918 - val_loss: 0.0635 - val_accuracy: 0.9823\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0367 - accuracy: 0.9908 - val_loss: 0.0661 - val_accuracy: 0.9831\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0376 - accuracy: 0.9905 - val_loss: 0.0688 - val_accuracy: 0.9815\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0375 - accuracy: 0.9910 - val_loss: 0.0630 - val_accuracy: 0.9815\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0355 - accuracy: 0.9920 - val_loss: 0.0633 - val_accuracy: 0.9854\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0358 - accuracy: 0.9918 - val_loss: 0.0651 - val_accuracy: 0.9823\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0368 - accuracy: 0.9918 - val_loss: 0.0699 - val_accuracy: 0.9823\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0386 - accuracy: 0.9897 - val_loss: 0.0629 - val_accuracy: 0.9831\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0365 - accuracy: 0.9910 - val_loss: 0.0630 - val_accuracy: 0.9815\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0351 - accuracy: 0.9923 - val_loss: 0.0641 - val_accuracy: 0.9831\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0362 - accuracy: 0.9915 - val_loss: 0.0655 - val_accuracy: 0.9831\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0350 - accuracy: 0.9923 - val_loss: 0.0626 - val_accuracy: 0.9831\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0354 - accuracy: 0.9923 - val_loss: 0.0630 - val_accuracy: 0.9823\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0355 - accuracy: 0.9918 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0375 - accuracy: 0.9918 - val_loss: 0.0633 - val_accuracy: 0.9831\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0358 - accuracy: 0.9923 - val_loss: 0.0632 - val_accuracy: 0.9831\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0354 - accuracy: 0.9915 - val_loss: 0.0678 - val_accuracy: 0.9838\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0385 - accuracy: 0.9905 - val_loss: 0.0736 - val_accuracy: 0.9808\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0395 - accuracy: 0.9897 - val_loss: 0.0619 - val_accuracy: 0.9831\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0369 - accuracy: 0.9915 - val_loss: 0.0664 - val_accuracy: 0.9823\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9910 - val_loss: 0.0670 - val_accuracy: 0.9808\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0369 - accuracy: 0.9923 - val_loss: 0.0624 - val_accuracy: 0.9838\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0366 - accuracy: 0.9918 - val_loss: 0.0634 - val_accuracy: 0.9831\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0353 - accuracy: 0.9918 - val_loss: 0.0637 - val_accuracy: 0.9838\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0343 - accuracy: 0.9920 - val_loss: 0.0619 - val_accuracy: 0.9815\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0344 - accuracy: 0.9931 - val_loss: 0.0631 - val_accuracy: 0.9846\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0346 - accuracy: 0.9926 - val_loss: 0.0628 - val_accuracy: 0.9838\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0344 - accuracy: 0.9928 - val_loss: 0.0628 - val_accuracy: 0.9838\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0351 - accuracy: 0.9926 - val_loss: 0.0616 - val_accuracy: 0.9846\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0356 - accuracy: 0.9918 - val_loss: 0.0653 - val_accuracy: 0.9823\n",
      "Epoch 344/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0349 - accuracy: 0.9915 - val_loss: 0.0609 - val_accuracy: 0.9854\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 0.9920 - val_loss: 0.0614 - val_accuracy: 0.9831\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0371 - accuracy: 0.9913 - val_loss: 0.0673 - val_accuracy: 0.9808\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0385 - accuracy: 0.9900 - val_loss: 0.0742 - val_accuracy: 0.9792\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0383 - accuracy: 0.9897 - val_loss: 0.0674 - val_accuracy: 0.9823\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0389 - accuracy: 0.9905 - val_loss: 0.0700 - val_accuracy: 0.9808\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0383 - accuracy: 0.9900 - val_loss: 0.0618 - val_accuracy: 0.9808\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0375 - accuracy: 0.9915 - val_loss: 0.0665 - val_accuracy: 0.9823\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0367 - accuracy: 0.9913 - val_loss: 0.0667 - val_accuracy: 0.9823\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0420 - accuracy: 0.9877 - val_loss: 0.0608 - val_accuracy: 0.9831\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0374 - accuracy: 0.9905 - val_loss: 0.0653 - val_accuracy: 0.9823\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0347 - accuracy: 0.9923 - val_loss: 0.0641 - val_accuracy: 0.9846\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0350 - accuracy: 0.9926 - val_loss: 0.0639 - val_accuracy: 0.9831\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0351 - accuracy: 0.9926 - val_loss: 0.0628 - val_accuracy: 0.9831\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0343 - accuracy: 0.9923 - val_loss: 0.0611 - val_accuracy: 0.9846\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0330 - accuracy: 0.9928 - val_loss: 0.0614 - val_accuracy: 0.9831\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0344 - accuracy: 0.9933 - val_loss: 0.0671 - val_accuracy: 0.9808\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0373 - accuracy: 0.9913 - val_loss: 0.0679 - val_accuracy: 0.9800\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0356 - accuracy: 0.9915 - val_loss: 0.0619 - val_accuracy: 0.9846\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0355 - accuracy: 0.9926 - val_loss: 0.0702 - val_accuracy: 0.9823\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9908 - val_loss: 0.0636 - val_accuracy: 0.9838\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0341 - accuracy: 0.9933 - val_loss: 0.0619 - val_accuracy: 0.9838\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0332 - accuracy: 0.9920 - val_loss: 0.0620 - val_accuracy: 0.9831\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0335 - accuracy: 0.9923 - val_loss: 0.0610 - val_accuracy: 0.9846\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0346 - accuracy: 0.9926 - val_loss: 0.0613 - val_accuracy: 0.9838\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0361 - accuracy: 0.9920 - val_loss: 0.0622 - val_accuracy: 0.9838\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0380 - accuracy: 0.9897 - val_loss: 0.0646 - val_accuracy: 0.9846\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0372 - accuracy: 0.9913 - val_loss: 0.0681 - val_accuracy: 0.9808\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0338 - accuracy: 0.9926 - val_loss: 0.0605 - val_accuracy: 0.9846\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0329 - accuracy: 0.9926 - val_loss: 0.0619 - val_accuracy: 0.9854\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0334 - accuracy: 0.9931 - val_loss: 0.0629 - val_accuracy: 0.9846\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0328 - accuracy: 0.9931 - val_loss: 0.0615 - val_accuracy: 0.9831\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0328 - accuracy: 0.9923 - val_loss: 0.0608 - val_accuracy: 0.9838\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0327 - accuracy: 0.9931 - val_loss: 0.0617 - val_accuracy: 0.9831\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0330 - accuracy: 0.9933 - val_loss: 0.0618 - val_accuracy: 0.9838\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0336 - accuracy: 0.9928 - val_loss: 0.0639 - val_accuracy: 0.9838\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0325 - accuracy: 0.9926 - val_loss: 0.0609 - val_accuracy: 0.9831\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0324 - accuracy: 0.9923 - val_loss: 0.0618 - val_accuracy: 0.9854\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0328 - accuracy: 0.9926 - val_loss: 0.0649 - val_accuracy: 0.9854\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0329 - accuracy: 0.9931 - val_loss: 0.0626 - val_accuracy: 0.9838\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0327 - accuracy: 0.9923 - val_loss: 0.0614 - val_accuracy: 0.9854\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 0.9926 - val_loss: 0.0617 - val_accuracy: 0.9831\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0329 - accuracy: 0.9931 - val_loss: 0.0620 - val_accuracy: 0.9831\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0339 - accuracy: 0.9923 - val_loss: 0.0633 - val_accuracy: 0.9831\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0356 - accuracy: 0.9918 - val_loss: 0.0785 - val_accuracy: 0.9792\n",
      "Epoch 389/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9913 - val_loss: 0.0630 - val_accuracy: 0.9854\n",
      "Epoch 390/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0362 - accuracy: 0.9920 - val_loss: 0.0619 - val_accuracy: 0.9831\n",
      "Epoch 391/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0340 - accuracy: 0.9913 - val_loss: 0.0726 - val_accuracy: 0.9823\n",
      "Epoch 392/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0346 - accuracy: 0.9920 - val_loss: 0.0629 - val_accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다.\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0489 - accuracy: 0.9815\n",
      "Test accuracy: 0.9815384745597839\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
